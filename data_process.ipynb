{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01-Fasttext\t\t\t        data_process.ipynb\r\n",
      " 02-TextCNN\t\t\t        data_utils.py\r\n",
      " 04-TextRCNN\t\t\t        images\r\n",
      "'05-Hierarchical Attention Networks '   README.md\r\n",
      " 06-memory-networks\t\t        utils.py\r\n",
      "'07-attention is all your need'\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/panxie/anaconda3/lib/python3.6/site-packages (3.2)\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import tarfile\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/panxie/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dbpedia():\n",
    "    dbpedia_url = \"https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\"\n",
    "\n",
    "    wget.download(dbpedia_url)\n",
    "    with tarfile.open(\"dbpedia_csv.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dbpedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01-Fasttext\t\t\t        data_utils.py\r\n",
      " 02-TextCNN\t\t\t        dbpedia_csv\r\n",
      " 04-TextRCNN\t\t\t        dbpedia_csv.tar.gz\r\n",
      "'05-Hierarchical Attention Networks '   images\r\n",
      " 06-memory-networks\t\t        README.md\r\n",
      "'07-attention is all your need'         utils.py\r\n",
      " data_process.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes.txt  readme.txt  test.csv  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls dbpedia_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"dbpedia_csv/train.csv\", names=[\"classes\", \"title\", \"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560000 entries, 0 to 559999\n",
      "Data columns (total 3 columns):\n",
      "classes    560000 non-null int64\n",
      "title      560000 non-null object\n",
      "content    560000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 12.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>The Unsigned Guide</td>\n",
       "      <td>The Unsigned Guide is an online contacts dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Rest of the world</td>\n",
       "      <td>Within sports and games played at the interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Globoforce</td>\n",
       "      <td>Globoforce is a multinational company co-head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Rompetrol</td>\n",
       "      <td>The Rompetrol Group N.V. is a Romanian oil co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Wave Accounting</td>\n",
       "      <td>Wave is the brand name for a suite of online ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classes                              title  \\\n",
       "0        1                   E. D. Abbott Ltd   \n",
       "1        1                     Schwan-Stabilo   \n",
       "2        1                         Q-workshop   \n",
       "3        1  Marvell Software Solutions Israel   \n",
       "4        1        Bergan Mercy Medical Center   \n",
       "5        1                 The Unsigned Guide   \n",
       "6        1                  Rest of the world   \n",
       "7        1                         Globoforce   \n",
       "8        1                          Rompetrol   \n",
       "9        1                    Wave Accounting   \n",
       "\n",
       "                                             content  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...  \n",
       "1   Schwan-STABILO is a German maker of pens for ...  \n",
       "2   Q-workshop is a Polish company located in Poz...  \n",
       "3   Marvell Software Solutions Israel known as RA...  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...  \n",
       "5   The Unsigned Guide is an online contacts dire...  \n",
       "6   Within sports and games played at the interna...  \n",
       "7   Globoforce is a multinational company co-head...  \n",
       "8   The Rompetrol Group N.V. is a Romanian oil co...  \n",
       "9   Wave is the brand name for a suite of online ...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词典\n",
    "- 使用nltk分词\n",
    "- 使用正则化去除特殊符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = train_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    # 正则化处理特殊字符\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "train_data = []\n",
    "for content in contents:\n",
    "    for word in word_tokenize(clean_str(content)):\n",
    "        \n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27518975\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbott',\n",
       " 'of',\n",
       " 'farnham',\n",
       " 'e',\n",
       " 'd',\n",
       " 'abbott',\n",
       " 'limited',\n",
       " 'was',\n",
       " 'a',\n",
       " 'british']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = collections.Counter(words)  # dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563260 <class 'collections.Counter'> 878840 772644 218\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counter), type(word_counter), word_counter['of'], word_counter['a'], word_counter['abbott'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = word_counter.most_common()  # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563260 [('the', 1666524), ('in', 939594), ('of', 878840), ('a', 772644), ('is', 761186)]\n"
     ]
    }
   ],
   "source": [
    "print(len(word_counter), word_counter[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "word_dict['<pad>'] = 0\n",
    "word_dict['<unk>'] = 1\n",
    "word_dict['<eos>'] = 2\n",
    "for word,_ in word_counter:\n",
    "    word_dict[word] = len(word_dict)  ## 机智"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['a'], word_dict['<unk>'],word_dict['<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到预处理后的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classes</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245182</th>\n",
       "      <td>7</td>\n",
       "      <td>Luther House</td>\n",
       "      <td>Luther House is a historic house at 177 Marke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31308</th>\n",
       "      <td>1</td>\n",
       "      <td>Cision</td>\n",
       "      <td>Cision AB is a Swedish software company. Cisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185153</th>\n",
       "      <td>5</td>\n",
       "      <td>Edward Baigent</td>\n",
       "      <td>Edward Baigent (22 June 1813 – 9 November 189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379573</th>\n",
       "      <td>10</td>\n",
       "      <td>Turritella</td>\n",
       "      <td>Turritella is a genus of medium-sized sea sna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154946</th>\n",
       "      <td>4</td>\n",
       "      <td>Peter Rafferty</td>\n",
       "      <td>Peter Rafferty was a Northern Irish footballe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        classes           title  \\\n",
       "245182        7    Luther House   \n",
       "31308         1          Cision   \n",
       "185153        5  Edward Baigent   \n",
       "379573       10      Turritella   \n",
       "154946        4  Peter Rafferty   \n",
       "\n",
       "                                                  content  \n",
       "245182   Luther House is a historic house at 177 Marke...  \n",
       "31308    Cision AB is a Swedish software company. Cisi...  \n",
       "185153   Edward Baigent (22 June 1813 – 9 November 189...  \n",
       "379573   Turritella is a genus of medium-sized sea sna...  \n",
       "154946   Peter Rafferty was a Northern Irish footballe...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 中每一行分词\n",
    "x = list(map(lambda d: word_tokenize(clean_str(d)), train_df[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['luther', 'house', 'is', 'a', 'historic', 'house', 'at', '177', 'market', 'street', 'in', 'swansea', 'massachusetts', 'the', 'house', 'was', 'built', 'in', '1740', 'and', 'added', 'to', 'the', 'national', 'historic', 'register', 'in', '1990'], ['cision', 'ab', 'is', 'a', 'swedish', 'software', 'company', 'cision', 'has', 'offices', 'in', 'europe', 'north', 'america', 'and', 'asia', 'as', 'of', '2011', 'it', 'has', 'revenues', 'of', 'sek', '1', 'billion']]\n"
     ]
    }
   ],
   "source": [
    "print(x[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6353, 68, 7, 6, 84, 68, 20, 12246, 866, 290, 4, 7232, 418, 3, 68, 11, 59, 4, 10947, 8, 530, 13, 3, 60, 84, 254, 4, 476]\n"
     ]
    }
   ],
   "source": [
    "# 以第一行为例，得到每个次在 word_dict 的index， 不存在的词赋予 word_dict['<unk>']\n",
    "print(list(map(lambda word: word_dict.get(word, word_dict['<unk>']), x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把 list(map(lambda word: word_dict.get(word, word_dict['<unk>']), d) 看作一个参数为d函数\n",
    "# list(map(lambda d: f, x))\n",
    "x = list(map(lambda d:list(map(lambda word: word_dict.get(word, word_dict['<unk>']), d)), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6353, 68, 7, 6, 84, 68, 20, 12246, 866, 290, 4, 7232, 418, 3, 68, 11, 59, 4, 10947, 8, 530, 13, 3, 60, 84, 254, 4, 476] 560000\n"
     ]
    }
   ],
   "source": [
    "print(x[0], len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加上结束词\n",
    "x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6353, 68, 7, 6, 84, 68, 20, 12246, 866, 290, 4, 7232, 418, 3, 68, 11, 59, 4, 10947, 8, 530, 13, 3, 60, 84, 254, 4, 476, 2] 560000\n"
     ]
    }
   ],
   "source": [
    "print(x[0], len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设定一个最长序列值\n",
    "x = list(map(lambda d:d[:30], x[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6353, 68, 7, 6, 84, 68, 20, 12246, 866, 290], [146319, 3087, 7, 6, 702, 840, 56, 146319, 32, 1121]]\n"
     ]
    }
   ],
   "source": [
    "print(x[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不足这个长度的补零\n",
    "x = list(map(lambda d: d + (15 - len(d)) * [word_dict[\"<pad>\"]], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6353, 68, 7, 6, 84, 68, 20, 12246, 866, 290, 0, 0, 0, 0, 0], [146319, 3087, 7, 6, 702, 840, 56, 146319, 32, 1121, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 真实标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(lambda d: d - 1, list(train_df[\"classes\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 0, 4, 9, 3, 1, 8, 1, 5, 1]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 迭代处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
